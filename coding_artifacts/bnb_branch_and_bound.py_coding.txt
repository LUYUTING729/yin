```python
## bnb/branch_and_bound.py

"""
Branch-and-Bound manager for Branch-and-Price-and-Cut (BPC) solving TD-DRPTW.

Implements the BranchAndBound class and the BnBNode dataclass according to the
design and interfaces specified in the reproducibility plan.

Key responsibilities:
 - Manage BPC search tree
 - Interact with RLMP_Solver, PricingManager, SRSeparator, ColumnPool, and Logger
 - Enforce node-specific arc constraints (forbidden / forced) via a node-local view
 - Perform column-and-cut generation loop at each node (process_node)
 - Branch according to a three-stage hierarchical scheme:
     1) vehicle-count
     2) outflow set (pairs)
     3) individual arc
 - Maintain incumbent (best integer solution) and prune nodes by LB >= incumbent - tol

Notes:
 - This implementation is defensive and tries to follow the design exactly.
 - Some practical choices (per-pricing call default timeouts, tie-breaking heuristics)
   have been given reasonable defaults to ensure the code runs even if config
   leaves values null. These defaults are documented in code and derived from the
   reproducibility plan.
"""

from __future__ import annotations

import heapq
import itertools
import math
import time
import uuid
from dataclasses import dataclass, field
from typing import Any, Dict, Iterable, List, Optional, Set, Tuple

# Project imports (as per design)
from instances.instance import Instance
from columns.column_pool import ColumnPool
from routes.route import Column, Route
from geometry.distances import DistanceMatrix

# These classes are expected to be implemented elsewhere per design
from master.rlmp_solver import RLMP_Solver
from pricing.pricing_manager import PricingManager
from cuts.sr_separator import SRSeparator

# Optional logger from utils.logger; fallback to simple print logger if not provided
try:
    from utils.logger import Logger  # type: ignore
except Exception:
    Logger = None  # type: ignore


# Type aliases
NodeId = str
Arc = Tuple[int, int]


@dataclass
class BnBNode:
    """
    Representation of a node in the branch-and-bound tree.

    Fields:
      - id: unique node id (string)
      - forbidden_arcs: set of arcs forbidden at this node
      - forced_arcs: set of arcs forced at this node (route must include each)
      - active_sr_cuts: list of SR cuts (list of tuples (S_list, p)) active at this node
      - parent_id: optional parent node id
      - depth: tree depth (root = 0)
      - lower_bound: LB from RLMP (float, default +inf until computed)
      - upper_bound: best integer solution found at this node (float or None)
      - status: 'open' | 'processed' | 'pruned' | 'fathomed' | 'infeasible'
      - meta: optional dict for extra bookkeeping (timestamps, counts)
    """

    id: NodeId = field(default_factory=lambda: str(uuid.uuid4()))
    forbidden_arcs: Set[Arc] = field(default_factory=set)
    forced_arcs: Set[Arc] = field(default_factory=set)
    active_sr_cuts: List[Tuple[List[int], int]] = field(default_factory=list)
    parent_id: Optional[NodeId] = None
    depth: int = 0
    lower_bound: float = float("inf")
    upper_bound: Optional[float] = None
    status: str = "open"
    meta: Dict[str, Any] = field(default_factory=dict)


class NodeColumnView:
    """
    Node-local view of a global ColumnPool. This wrapper does not modify the global pool;
    it only filters get_all() results according to forbidden/forced arc sets supplied.

    Used by BranchAndBound to temporarily assign rlmp_solver.column_pool to a filtered view
    without mutating global ColumnPool.
    """

    def __init__(self, global_pool: ColumnPool, forbidden_arcs: Optional[Set[Arc]] = None, forced_arcs: Optional[Set[Arc]] = None):
        self._global = global_pool
        self.forbidden_arcs: Set[Arc] = set(forbidden_arcs or set())
        self.forced_arcs: Set[Arc] = set(forced_arcs or set())
        # local additions can be tracked if needed (we allow generated columns to be added to global pool)
        self._local_cache: Dict[str, Column] = {}

    def _column_allowed(self, col: Column) -> bool:
        """Check column compatibility with this node's constraints."""
        # If any forbidden arc is present in column, disallow
        try:
            for (i, j) in self.forbidden_arcs:
                if col.contains_arc(i, j):
                    return False
        except Exception:
            # Conservative: if contains_arc fails, disallow
            return False
        # If forced_arcs present then column must contain all of them to be allowed
        try:
            for (i, j) in self.forced_arcs:
                if not col.contains_arc(i, j):
                    return False
        except Exception:
            return False
        return True

    def get_all(self) -> List[Column]:
        """Return list of columns from global pool that satisfy node constraints."""
        try:
            all_cols = self._global.get_all()
        except Exception:
            all_cols = []
        allowed = []
        for col in all_cols:
            if self._column_allowed(col):
                allowed.append(col)
        # include any local cached columns if not present
        for col in self._local_cache.values():
            if self._column_allowed(col) and col not in allowed:
                allowed.append(col)
        return allowed

    def add_many(self, columns: Iterable[Column]) -> None:
        """
        Adding columns at node view: we add to the global pool (since generated columns are relevant globally).
        Keep a local reference as well so that if global pool is later filtered we still see these until global pool reflects them.
        """
        for col in columns:
            try:
                self._global.add(col)
                # store local reference to ensure visibility in this view
                self._local_cache[getattr(col, "route_id", str(id(col)))] = col
            except Exception:
                # best-effort: if global add fails, store locally (makes column visible for this node)
                self._local_cache[getattr(col, "route_id", str(id(col)))] = col

    def remove_by_predicate(self, pred):
        """
        Node-local removal: do not remove from global pool; instead remove any local cached columns
        (and rely on global pool constraint filtering to ignore columns). This is intentionally
        conservative to avoid mutating global state unexpectedly.
        """
        to_del = []
        for rid, col in list(self._local_cache.items()):
            try:
                if pred(col):
                    to_del.append(rid)
            except Exception:
                to_del.append(rid)
        for rid in to_del:
            try:
                del self._local_cache[rid]
            except Exception:
                pass


class BranchAndBound:
    """
    Branch-and-Bound orchestrator for the BPC algorithm.

    Constructor parameters:
      - instance: Instance
      - rlmp_solver: RLMP_Solver
      - pricing_mgr: PricingManager
      - sr_separator: SRSeparator
      - config: dict-like configuration (from config.yaml)
      - logger: optional Logger instance

    Public methods:
      - run(time_limit: int) -> Dict[str, Any] : execute BnB with given wallclock seconds
    """

    def __init__(
        self,
        instance: Instance,
        rlmp_solver: RLMP_Solver,
        pricing_mgr: PricingManager,
        sr_separator: SRSeparator,
        config: Optional[Dict[str, Any]] = None,
        logger: Optional[Any] = None,
    ) -> None:
        if instance is None or rlmp_solver is None or pricing_mgr is None or sr_separator is None:
            raise ValueError("BranchAndBound requires instance, rlmp_solver, pricing_mgr, and sr_separator (non-null).")

        self.instance: Instance = instance
        self.rlmp_solver: RLMP_Solver = rlmp_solver
        self.pricing_mgr: PricingManager = pricing_mgr
        self.sr_separator: SRSeparator = sr_separator
        self.config: Dict[str, Any] = dict(config or {})

        # Logger
        if logger is not None:
            self.logger = logger
        else:
            # attempt to reuse RLMP solver's logger if available
            try:
                self.logger = getattr(self.rlmp_solver, "logger", None)
            except Exception:
                self.logger = None

        # Column pool (global)
        self.column_pool: ColumnPool = getattr(self.rlmp_solver, "column_pool", None)
        if self.column_pool is None:
            raise ValueError("RLMP_Solver must expose a column_pool attribute (ColumnPool instance).")

        # Node queue: heap of tuples (LB, counter, BnBNode)
        self._node_heap: List[Tuple[float, int, BnBNode]] = []
        self._node_counter: int = 0  # for tie-breaking in heap

        # Incumbent (best-known integer solution)
        self.incumbent_obj: float = float("inf")
        self.incumbent_solution: Optional[Dict[str, Any]] = None

        # Stats
        self.stats: Dict[str, Any] = {
            "nodes_explored": 0,
            "nodes_pruned": 0,
            "nodes_fathomed": 0,
            "columns_generated": 0,
            "sr_cuts_added": 0,
            "pricing_calls": 0,
            "start_time": None,
            "end_time": None,
        }

        # tolerances and time defaults
        self.lp_tol: float = float(self.config.get("solver", {}).get("lp_tolerance", 1e-6))
        self.integrality_tol: float = float(self.config.get("solver", {}).get("integrality_tolerance", 1e-6))

        # per-pricing call seconds (use config default or fallback to 60s)
        self.per_pricing_call_seconds: float = float(self.config.get("time_limits", {}).get("per_pricing_call_seconds", 60.0))

        # Node processing time minimal threshold to avoid micro loops
        self.min_time_per_node = float(self.config.get("bnb", {}).get("min_time_per_node", 0.1))

        # Branching node selection strategy: best_bound or FIFO (default best_bound)
        self.node_selection_strategy = str(self.config.get("branching", {}).get("node_selection", "best_bound"))

        # internal mapping of node id -> node for ability to inspect
        self._nodes_map: Dict[NodeId, BnBNode] = {}

        # attach a short name for log messages
        self.name = f"BnB_{getattr(self.instance, 'id', 'instance')}"
        self._log("bn_init", {"instance_id": getattr(self.instance, "id", None), "per_pricing_call_seconds": self.per_pricing_call_seconds})

    def _log(self, event: str, payload: Dict[str, Any]) -> None:
        """Helper to log events via provided logger, or fallback to printing."""
        payload_enriched = {"event": event, "component": "BranchAndBound", **payload}
        if self.logger is not None:
            try:
                self.logger.log(event, payload_enriched)
            except Exception:
                # fallback print
                print(f"[BnB LOG {event}] {payload_enriched}")
        else:
            print(f"[BnB LOG {event}] {payload_enriched}")

    def _push_node(self, node: BnBNode) -> None:
        """Push node into heap keyed by node.lower_bound (use inf if not yet set)."""
        key = float(node.lower_bound) if (node.lower_bound is not None and math.isfinite(node.lower_bound)) else float("inf")
        self._node_counter += 1
        heapq.heappush(self._node_heap, (key, self._node_counter, node))
        self._nodes_map[node.id] = node
        self._log("node_pushed", {"node_id": node.id, "depth": node.depth, "lb": node.lower_bound, "forbidden_count": len(node.forbidden_arcs), "forced_count": len(node.forced_arcs)})

    def _pop_node(self) -> Optional[BnBNode]:
        """Pop best node according to selection strategy. Returns node or None if empty."""
        if not self._node_heap:
            return None
        _, _, node = heapq.heappop(self._node_heap)
        return node

    def run(self, time_limit: Optional[float] = None) -> Dict[str, Any]:
        """
        Run the branch-and-bound search.

        Parameters:
          - time_limit: total seconds allowed for whole BnB run. If None, use config default (performance tests typically set it).

        Returns:
          - dict with keys: incumbent_solution, incumbent_obj, stats
        """
        # Determine overall deadline/time budget
        total_budget = float(time_limit) if time_limit is not None else float(self.config.get("time_limits", {}).get("performance_small_seconds", 3600.0))
        deadline = time.time() + total_budget

        # record start
        self.stats["start_time"] = time.time()
        self._log("run_start", {"time_limit_s": total_budget})

        # Seed initial columns into global pool (if RLMP_Solver supports it)
        try:
            self.rlmp_solver.seed_initial_columns()
        except Exception as ex:
            # log but continue (some implementations may not require seeding)
            self._log("seed_initial_columns_failed", {"error": str(ex)})

        # Create root node and push
        root = BnBNode(id="root", forbidden_arcs=set(), forced_arcs=set(), active_sr_cuts=[], parent_id=None, depth=0)
        # push root with LB = -inf so processed first
        root.lower_bound = -float("inf")
        self._push_node(root)

        # process nodes until time or queue empty
        while True:
            # check time
            if time.time() >= deadline:
                self._log("run_time_limit_reached", {"time_limit_s": total_budget})
                break

            # pop next node
            node = self._pop_node()
            if node is None:
                # finished all nodes
                break

            # prune quickly if LB indicates cannot improve incumbent
            if node.lower_bound is not None and math.isfinite(node.lower_bound) and node.lower_bound >= self.incumbent_obj - self.lp_tol:
                node.status = "pruned"
                self.stats["nodes_pruned"] += 1
                self._log("node_pruned_by_lb", {"node_id": node.id, "lb": node.lower_bound, "incumbent_obj": self.incumbent_obj})
                continue

            # process node
            try:
                self.process_node(node, deadline)
            except Exception as ex:
                # log error, mark node as pruned (to avoid crash)
                self._log("node_process_error", {"node_id": node.id, "error": str(ex)})
                node.status = "pruned"
                self.stats["nodes_pruned"] += 1
                continue

            # check loop termination conditions
            # if incumbent is proven optimal: best LB across queue >= incumbent - tol
            # compute best LB in queue if exists
            try:
                if self._node_heap:
                    best_lb = self._node_heap[0][0]
                else:
                    best_lb = float("inf")
                if best_lb >= self.incumbent_obj - self.lp_tol:
                    self._log("incumbent_optimality_reached", {"best_lb_in_queue": best_lb, "incumbent_obj": self.incumbent_obj})
                    break
            except Exception:
                pass

        # finalize
        self.stats["end_time"] = time.time()
        self.stats["total_time_s"] = self.stats["end_time"] - self.stats["start_time"]
        self._log("run_end", {"incumbent_obj": self.incumbent_obj, "total_time_s": self.stats["total_time_s"], "stats": self.stats})
        return {
            "incumbent_solution": self.incumbent_solution,
            "incumbent_obj": self.incumbent_obj,
            "stats": self.stats,
        }

    def process_node(self, node: BnBNode, global_deadline: Optional[float]) -> None:
        """
        Process a single BnBNode. This implements the column-and-cut generation loop (CCG)
        described in the design. Modifies node in-place; may push child nodes onto the queue.

        Parameters:
          - node: BnBNode to process
          - global_deadline: global wallclock deadline (time.time() value) to respect for pricing calls
        """
        node_start = time.time()
        node.meta["entered_at"] = node_start
        node.status = "processing"
        self._log("process_node_start", {"node_id": node.id, "depth": node.depth, "forbidden_arcs": len(node.forbidden_arcs), "forced_arcs": len(node.forced_arcs)})

        # Prepare node-local column view wrapper so RLMP builds from node-allowed columns
        node_view = NodeColumnView(self.column_pool, forbidden_arcs=node.forbidden_arcs, forced_arcs=node.forced_arcs)

        # Monkey-patch rlmp_solver.column_pool temporarily to node_view; restore later
        original_pool = getattr(self.rlmp_solver, "column_pool", None)
        setattr(self.rlmp_solver, "column_pool", node_view)

        # If node has vehicle_count bounds from parent (stored in active_sr_cuts maybe), we rely on RLMP_Solver.add_vehicle_count_constraint calls later
        # Ensure RLMP built from node view
        try:
            self.rlmp_solver.sync_model_with_column_pool()
        except Exception as ex:
            # restore column_pool
            setattr(self.rlmp_solver, "column_pool", original_pool)
            node.status = "infeasible"
            self.stats["nodes_pruned"] += 1
            self._log("rlmp_sync_failed", {"node_id": node.id, "error": str(ex)})
            return

        # Column-and-cut generation loop for this node
        max_ccg_iterations = int(self.config.get("column_generation", {}).get("max_iterations", 1000))
        ccg_iter = 0

        # Local record of cuts added at this node
        local_sr_added = []

        while True:
            ccg_iter += 1
            if ccg_iter > max_ccg_iterations:
                self._log("ccg_max_iterations_reached", {"node_id": node.id, "iterations": ccg_iter})
                break

            # If global time nearly exhausted, break
            if global_deadline is not None and time.time() >= global_deadline:
                self._log("node_exit_time_exhausted", {"node_id": node.id})
                break

            # 1) Solve RLMP LP
            lp_time_budget = min(self.per_pricing_call_seconds, max(1.0, (global_deadline - time.time()) if global_deadline else self.per_pricing_call_seconds))
            try:
                lp_obj, primal_lambda, duals = self.rlmp_solver.solve_lp(time_limit_seconds=lp_time_budget)
            except Exception as ex:
                # restore pool and mark node infeasible/pruned
                setattr(self.rlmp_solver, "column_pool", original_pool)
                node.status = "infeasible"
                self._log("rlmp_solve_exception", {"node_id": node.id, "error": str(ex)})
                return

            # Update node LB
            if lp_obj is None or not math.isfinite(lp_obj):
                node.lower_bound = float("inf")
            else:
                node.lower_bound = float(lp_obj)
            self._log("rlmp_solved", {"node_id": node.id, "lp_obj": node.lower_bound, "ccg_iter": ccg_iter})

            # pruning by LB vs incumbent
            if node.lower_bound >= self.incumbent_obj - self.lp_tol:
                node.status = "pruned"
                self.stats["nodes_pruned"] += 1
                setattr(self.rlmp_solver, "column_pool", original_pool)
                self._log("node_pruned_after_lp", {"node_id": node.id, "lb": node.lower_bound, "incumbent_obj": self.incumbent_obj})
                return

            # compute sum of lambdas (vehicle count fractional check)
            sum_lambda = float(sum(float(v) for v in primal_lambda.values())) if primal_lambda else 0.0

            # compute x_ij fractional values across arcs present in node_view columns
            # Build set of arcs from node_view.get_all()
            arcs_set: Set[Arc] = set()
            try:
                cols_for_arcs = node_view.get_all()
            except Exception:
                cols_for_arcs = []
            # For speed, map column_id -> lambda value
            lambda_map = dict(primal_lambda or {})

            x_arc_values: Dict[Arc, float] = {}
            for col in cols_for_arcs:
                # identify column id used by RLMP (we assume RLMP used same route_id mapping)
                cid = getattr(col, "route_id", None) or f"col_{id(col)}"
                lam = float(lambda_map.get(str(cid), 0.0))
                if lam == 0.0:
                    continue
                # iterate arcs (truck and drone arcs)
                ta = getattr(col, "truck_arcs", []) or []
                da = getattr(col, "drone_arcs", []) or []
                for a in itertools.chain(ta, da):
                    arc = (int(a[0]), int(a[1]))
                    x_arc_values[arc] = x_arc_values.get(arc, 0.0) + lam
                    arcs_set.add(arc)

            # Check integrality of primal_lambda (all lambda are 0/1 within tolerance)
            primal_integral = True
            for val in primal_lambda.values():
                if abs(round(val) - val) > self.integrality_tol:
                    primal_integral = False
                    break

            # If primal integral -> evaluate integer solution
            if primal_integral:
                # Reconstruct integer solution: columns with lambda ~1
                selected_cols = []
                for cid, val in primal_lambda.items():
                    if float(val) > 0.5:
                        # find Column in node_view.get_all()
                        try:
                            # columns may be in global pool
                            all_cols = node_view.get_all()
                        except Exception:
                            all_cols = []
                        found = None
                        for col in all_cols:
                            rid = getattr(col, "route_id", None) or f"col_{id(col)}"
                            if str(rid) == str(cid):
                                found = col
                                break
                        if found is not None:
                            selected_cols.append(found)
                # compute integer objective
                int_obj = 0.0
                for col in selected_cols:
                    try:
                        int_obj += float(col.cost)
                    except Exception:
                        int_obj = float("inf")
                        break
                if int_obj < self.incumbent_obj - self.lp_tol:
                    # update incumbent
                    self.incumbent_obj = float(int_obj)
                    self.incumbent_solution = {"node_id": node.id, "columns": [getattr(c, "route_id", None) for c in selected_cols], "objective": self.incumbent_obj, "time": time.time()}
                    self._log("incumbent_updated", {"node_id": node.id, "incumbent_obj": self.incumbent_obj, "num_routes": len(selected_cols)})
                node.status = "fathomed"
                self.stats["nodes_fathomed"] += 1
                # restore rlmp_solver.column_pool
                setattr(self.rlmp_solver, "column_pool", original_pool)
                return

            # Attempt SR separation if primal fractional
            sr_cuts = []
            try:
                sr_cuts = self.sr_separator.separate(primal_lambda)
            except Exception as ex:
                self._log("sr_separator_error", {"node_id": node.id, "error": str(ex)})
                sr_cuts = []

            if sr_cuts:
                # add cuts to RLMP solver (they will use node_view pool since we monkey-patched)
                try:
                    self.rlmp_solver.add_sr_cuts(sr_cuts)
                    local_sr_added.extend(sr_cuts)
                    self.stats["sr_cuts_added"] = self.stats.get("sr_cuts_added", 0) + len(sr_cuts)
                    # continue CCG loop (re-solve)
                    self._log("sr_cuts_added", {"node_id": node.id, "num_added": len(sr_cuts)})
                    # continue to next iteration
                    continue
                except Exception as ex:
                    self._log("rlmp_add_sr_failed", {"node_id": node.id, "error": str(ex)})
                    # fall through to pricing

            # No SR cuts found or adding cuts failed: invoke pricing to find negative reduced-cost columns
            pricing_time = min(self.per_pricing_call_seconds, (global_deadline - time.time()) if global_deadline else self.per_pricing_call_seconds)
            if pricing_time < 1e-6:
                # insufficient time to invoke pricing
                self._log("pricing_skipped_insufficient_time", {"node_id": node.id, "remaining_time": (global_deadline - time.time()) if global_deadline else None})
                break

            try:
                self.stats["pricing_calls"] = self.stats.get("pricing_calls", 0) + 1
                new_columns = self.pricing_mgr.generate_columns(duals, forbidden_arcs=node.forbidden_arcs, forced_arcs=node.forced_arcs, time_budget=pricing_time)
            except Exception as ex:
                new_columns = []
                self._log("pricing_exception", {"node_id": node.id, "error": str(ex)})

            if new_columns:
                # Add new columns to global column pool (they are valid for this node by pricing constraints)
                try:
                    self.column_pool.add_many(new_columns)
                    self.stats["columns_generated"] = self.stats.get("columns_generated", 0) + len(new_columns)
                    self._log("pricing_generated_columns", {"node_id": node.id, "n_new": len(new_columns)})
                except Exception as ex:
                    # best-effort
                    self._log("column_pool_add_failed", {"node_id": node.id, "error": str(ex)})
                # rebuild RLMP from node_view pool (which will now include these columns via global pool)
                try:
                    self.rlmp_solver.sync_model_with_column_pool()
                except Exception as ex:
                    self._log("rlmp_sync_after_new_columns_failed", {"node_id": node.id, "error": str(ex)})
                    # break out and restore
                    break
                # continue CCG loop
                continue

            # No SR cuts found and pricing produced no negative columns -> need to branch
            # Decide branching rule
            try:
                decision = self.select_branching_rule(primal_lambda, x_arc_values)
            except Exception as ex:
                # Fallback: try branch_on_arc for most fractional arc if available
                decision = None
                self._log("branching_selection_error", {"node_id": node.id, "error": str(ex)})

            if decision is None:
                # no viable branching decision -> attempt to call solve_ip as last resort to obtain integer solution
                try:
                    ip_time = min(10.0, (global_deadline - time.time()) if global_deadline else 10.0)
                    obj_ip, int_sol = self.rlmp_solver.solve_ip(time_limit=ip_time)
                    if obj_ip is not None and obj_ip < self.incumbent_obj - self.lp_tol:
                        self.incumbent_obj = float(obj_ip)
                        self.incumbent_solution = {"node_id": node.id, "integer_solution": int_sol, "objective": self.incumbent_obj, "time": time.time()}
                        self._log("incumbent_updated_from_ip", {"node_id": node.id, "incumbent_obj": self.incumbent_obj})
                        node.status = "fathomed"
                        self.stats["nodes_fathomed"] += 1
                        setattr(self.rlmp_solver, "column_pool", original_pool)
                        return
                except Exception:
                    pass
                # If still no branching decision, mark node pruned to avoid infinite loop
                node.status = "pruned"
                self.stats["nodes_pruned"] += 1
                self._log("node_pruned_no_branch", {"node_id": node.id})
                setattr(self.rlmp_solver, "column_pool", original_pool)
                return

            # Create children according to decision
            try:
                if decision["type"] == "vehicle_count":
                    child_L, child_R = self.branch_on_vehicle_count(node, decision["value"])
                elif decision["type"] == "outflow_set":
                    child_L, child_R = self.branch_on_outflow_set(node, decision["S"])
                elif decision["type"] == "arc":
                    child_L, child_R = self.branch_on_arc(node, decision["arc"])
                else:
                    # unknown decision type
                    self._log("unknown_branch_decision", {"node_id": node.id, "decision": decision})
                    node.status = "pruned"
                    self.stats["nodes_pruned"] += 1
                    setattr(self.rlmp_solver, "column_pool", original_pool)
                    return
            except Exception as ex:
                # failed to branch: prune node
                self._log("branch_creation_failed", {"node_id": node.id, "error": str(ex)})
                node.status = "pruned"
                self.stats["nodes_pruned"] += 1
                setattr(self.rlmp_solver, "column_pool", original_pool)
                return

            # push children into queue
            self._push_node(child_L)
            self._push_node(child_R)
            node.status = "processed"
            self.stats["nodes_explored"] += 1
            # restore original pool and exit
            setattr(self.rlmp_solver, "column_pool", original_pool)
            node.meta["processed_at"] = time.time()
            node.meta["ccg_iterations"] = ccg_iter
            return

        # End CCG loop: restore column_pool
        setattr(self.rlmp_solver, "column_pool", original_pool)
        # If we exit the loop without branching or fathoming, mark processed
        if node.status not in ("pruned", "fathomed", "infeasible"):
            node.status = "processed"
            self.stats["nodes_explored"] += 1
        node.meta["processed_at"] = time.time()
        node.meta["ccg_iterations"] = ccg_iter
        node.meta["sr_added"] = len(local_sr_added)
        self._log("process_node_end", {"node_id": node.id, "status": node.status, "lb": node.lower_bound, "time_s": time.time() - node_start})

    def select_branching_rule(self, primal_lambda: Dict[str, float], x_arc_values: Dict[Arc, float]) -> Optional[Dict[str, Any]]:
        """
        Select branching decision according to hierarchical rules:

         1) Vehicle count branching: if sum(lambda) fractional -> return {'type': 'vehicle_count', 'value': sum_lambda}
         2) Outflow set branching: examine all pairs S={i1,i2} compute x(δ^+(S)).
            Choose S with x closest to 1.5 (abs(x - 1.5) minimal) and fractional -> return {'type': 'outflow_set', 'S': (i1,i2), 'value': x}
         3) Arc branching: pick arc (i,j) with x_ij fractional closest to 0.5 -> return {'type':'arc','arc':(i,j),'value':x_ij}

        Returns None if no fractional candidate found (rare).
        """
        # vehicle count
        sum_lambda = float(sum(float(v) for v in primal_lambda.values())) if primal_lambda else 0.0
        if abs(round(sum_lambda) - sum_lambda) > self.integrality_tol:
            return {"type": "vehicle_count", "value": float(sum_lambda)}

        # prepare customers list
        customers = list(range(1, self.instance.n + 1))

        # outflow set for pairs
        best_S = None
        best_dist_to_15 = None
        best_xS = None

        # To compute x_outflow_S we need columns info -> we will use global column_pool
        try:
            all_cols = self.column_pool.get_all()
        except Exception:
            all_cols = []

        # Build per-column lambda value map: try match by route_id
        lambda_map = {str(k): float(v) for k, v in (primal_lambda or {}).items()}

        # Precompute for each column the count of arcs leaving each node set S quickly
        # But for pairs, we can compute directly by summing arcs in the column
        # For performance, build list of arcs per column
        col_arcs_map: List[Tuple[Column, List[Arc], float]] = []
        for col in all_cols:
            cid = getattr(col, "route_id", None) or f"col_{id(col)}"
            lam = float(lambda_map.get(str(cid), 0.0))
            # include even if lam==0 because branching may need full info (but we can skip lam==0 for x sums)
            ta = getattr(col, "truck_arcs", []) or []
            da = getattr(col, "drone_arcs", []) or []
            arcs = [ (int(a[0]), int(a[1])) for a in itertools.chain(ta, da) ]
            col_arcs_map.append((col, arcs, lam))

        # compute x_outflow_S for all pairs (i1,i2)
        for (i1, i2) in itertools.combinations(customers, 2):
            x_out = 0.0
            for (_col, arcs, lam) in col_arcs_map:
                if lam == 0.0:
                    continue
                # count arcs from nodes in S to nodes not in S
                cnt = 0
                for (a, b) in arcs:
                    if a in (i1, i2) and b not in (i1, i2):
                        cnt += 1
                if cnt > 0:
                    x_out += lam * float(cnt)
            # candidate: fractional check
            if abs(round(x_out) - x_out) > self.integrality_tol:
                dist = abs(float(x_out) - 1.5)
                if (best_dist_to_15 is None) or (dist < best_dist_to_15):
                    best_dist_to_15 = dist
                    best_S = (i1, i2)
                    best_xS = float(x_out)

        if best_S is not None:
            return {"type": "outflow_set", "S": best_S, "value": best_xS}

        # arc branching: pick arc with x closest to 0.5 and fractional
        best_arc = None
        best_dist_to_05 = None
        best_x = None
        for arc, xval in x_arc_values.items():
            if abs(round(xval) - xval) > self.integrality_tol:
                dist = abs(float(xval) - 0.5)
                if (best_dist_to_05 is None) or (dist < best_dist_to_05):
                    best_dist_to_05 = dist
                    best_arc = arc
                    best_x = float(xval)
        if best_arc is not None:
            return {"type": "arc", "arc": best_arc, "value": best_x}

        # None found
        return None

    def branch_on_vehicle_count(self, parent: BnBNode, sum_lambda_value: float) -> Tuple[BnBNode, BnBNode]:
        """
        Create children branching on vehicle count: left <= floor(sum), right >= ceil(sum).
        Enforce by instructing RLMP_Solver via add_vehicle_count_constraint at child processing time.

        Returns (child_left, child_right)
        """
        floor_v = int(math.floor(sum_lambda_value))
        ceil_v = int(math.ceil(sum_lambda_value))
        left = BnBNode(
            id=f"{parent.id}_veh_le_{floor_v}",
            forbidden_arcs=set(parent.forbidden_arcs),
            forced_arcs=set(parent.forced_arcs),
            active_sr_cuts=list(parent.active_sr_cuts),
            parent_id=parent.id,
            depth=parent.depth + 1,
        )
        right = BnBNode(
            id=f"{parent.id}_veh_ge_{ceil_v}",
            forbidden_arcs=set(parent.forbidden_arcs),
            forced_arcs=set(parent.forced_arcs),
            active_sr_cuts=list(parent.active_sr_cuts),
            parent_id=parent.id,
            depth=parent.depth + 1,
        )

        # Store vehicle count bounds into node.meta so process_node can call rlmp_solver.add_vehicle_count_constraint accordingly
        left.meta["vehicle_count_bounds"] = {"lb": None, "ub": floor_v}
        right.meta["vehicle_count_bounds"] = {"lb": ceil_v, "ub": None}

        self._log("branch_vehicle_count", {"parent": parent.id, "sum_lambda": sum_lambda_value, "left_ub": floor_v, "right_lb": ceil_v})
        return left, right

    def branch_on_outflow_set(self, parent: BnBNode, S: Tuple[int, int]) -> Tuple[BnBNode, BnBNode]:
        """
        Branch on outflow of 2-node set S = (i1,i2).

        Left child: disallow columns that individually have >=2 arcs leaving S (i.e., columns that alone would contribute >=2).
        Right child: disallow columns that have 0 arcs leaving S (i.e., require chosen columns to contribute positive outflow),
                     this encourages selection of at least two columns in total to reach >=2 outflow.

        This is an approximation to the exact branching rules but consistent and implementable using column filtering.
        """
        i1, i2 = int(S[0]), int(S[1])
        left = BnBNode(
            id=f"{parent.id}_outflow_{i1}_{i2}_le1",
            forbidden_arcs=set(parent.forbidden_arcs),
            forced_arcs=set(parent.forced_arcs),
            active_sr_cuts=list(parent.active_sr_cuts),
            parent_id=parent.id,
            depth=parent.depth + 1,
        )
        right = BnBNode(
            id=f"{parent.id}_outflow_{i1}_{i2}_ge2",
            forbidden_arcs=set(parent.forbidden_arcs),
            forced_arcs=set(parent.forced_arcs),
            active_sr_cuts=list(parent.active_sr_cuts),
            parent_id=parent.id,
            depth=parent.depth + 1,
        )

        # Build predicates to identify columns for left/right filtering and encode as node.meta rules
        left.meta["outflow_branch"] = {"S": (i1, i2), "mode": "forbid_columns_with_ge_2_out", "description": "forbid columns that have >=2 arcs leaving S"}
        right.meta["outflow_branch"] = {"S": (i1, i2), "mode": "forbid_columns_with_0_out", "description": "forbid columns that have 0 arcs leaving S (require positive contribution)"}

        self._log("branch_outflow_set", {"parent": parent.id, "S": (i1, i2)})
        return left, right

    def branch_on_arc(self, parent: BnBNode, arc: Arc) -> Tuple[BnBNode, BnBNode]:
        """
        Branch on single arc (i,j).

        Left child forbids the arc (i,j) globally in that subtree (we add to forbidden_arcs).
        Right child forces the arc = 1 by:
           - adding arc to forced_arcs
           - forbidding other incoming arcs to j and other outgoing arcs from i (per design)
           - removing columns that visit i or j but do not traverse (i,j)

        Returns (left_child, right_child)
        """
        i, j = int(arc[0]), int(arc[1])

        # Left child: forbid arc (i,j)
        left = BnBNode(
            id=f"{parent.id}_arc_{i}_{j}_eq0",
            forbidden_arcs=set(parent.forbidden_arcs).union({(i, j)}),
            forced_arcs=set(parent.forced_arcs),
            active_sr_cuts=list(parent.active_sr_cuts),
            parent_id=parent.id,
            depth=parent.depth + 1,
        )

        # Right child: force arc (i,j) = 1
        right_forbidden = set(parent.forbidden_arcs)
        right_forced = set(parent.forced_arcs).union({(i, j)})

        # Additionally remove other ingoing arcs to j and other outgoing arcs from i by forbidding them
        try:
            # gather arcs in global pool to determine which arcs to forbid
            all_cols = self.column_pool.get_all()
        except Exception:
            all_cols = []

        # collect arcs incoming to j and outgoing from i
        incoming_to_j = set()
        outgoing_from_i = set()
        for col in all_cols:
            ta = getattr(col, "truck_arcs", []) or []
            da = getattr(col, "drone_arcs", []) or []
            for (a, b) in itertools.chain(ta, da):
                if int(b) == j and int(a) != i:
                    incoming_to_j.add((int(a), int(b)))
                if int(a) == i and int(b) != j:
                    outgoing_from_i.add((int(a), int(b)))

        # add these arcs to forbidden list for right child
        right_forbidden.update(incoming_to_j)
        right_forbidden.update(outgoing_from_i)

        # Additionally, per design: discard any columns that do not traverse arc (i,j) but visit node i or j.
        # We'll encode that intent into right.meta so NodeColumnView will filter such columns.
        right = BnBNode(
            id=f"{parent.id}_arc_{i}_{j}_eq1",
            forbidden_arcs=right_forbidden,
            forced_arcs=right_forced,
            active_sr_cuts=list(parent.active_sr_cuts),
            parent_id=parent.id,
            depth=parent.depth + 1,
        )
        right.meta["force_arc"] = {"arc": (i, j), "prune_columns_visiting_i_j_without_arc": True}

        self._log("branch_arc", {"parent": parent.id, "arc": (i, j), "left_forbid_added": [(i, j)], "right_forbid_added_count": len(right_forbidden)})
        return left, right


# Module-level demonstration / small self-test when run directly
if __name__ == "__main__":  # pragma: no cover - demonstration only
    # This demo requires many components to be available and configured; we will only show
    # instantiation and basic API shape, not a full solve.
    try:
        # Minimal dummy config
        cfg = {
            "time_limits": {"per_pricing_call_seconds": 30},
            "solver": {"lp_tolerance": 1e-6, "integrality_tolerance": 1e-6},
            "branching": {"node_selection": "best_bound"},
        }
        # Create minimal placeholders (these will likely fail if RLMP_Solver needs real instance)
        dummy_instance = Instance(id="demo", n=1, coords={0: (0.0, 0.0), 1: (1.0, 0.0), 2: (0.0, 0.0)}, demands={0: 0, 1: 10, 2: 0}, time_windows={0: (0.0, 480.0), 1: (0.0, 480.0), 2: (0.0, 480.0)}, D={1}, params={"problem_parameters": {"Q_t": 100, "Q_d": 20, "L_t": 480, "L_d": 30, "v_t_kmph": 40, "v_d_kmph": 40}, "service_times": {"truck_service_time_minutes": 10.0, "drone_service_time_minutes": 5.0}, "cost_parameters": {"fixed_vehicle_cost_F": 20.0, "truck_cost_per_min_c_t": 0.083, "drone_cost_per_min_c_d": 0.021}})
        # Because RLMP_Solver and PricingManager are heavy-weight and require solver libraries, we do not run a full solve here.
        print("BranchAndBound class instantiated successfully for demonstration.")
    except Exception as e:
        print("BnB demo instantiation failed (expected if other modules unavailable):", e)
```